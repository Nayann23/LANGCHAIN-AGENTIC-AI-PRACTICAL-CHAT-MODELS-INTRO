{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d555c3",
   "metadata": {},
   "source": [
    "#### **Getting Started With Langchanin**\n",
    "\n",
    "- **Simple LLM calls with Streaming**\n",
    "- **Dynamic prompt templates (translation app)**\n",
    "- **Building Chains (story generator with analysis)**\n",
    "- **Conversational Q&A assistant with memory**\n",
    "- **Tool integration (calculator & weather)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee73c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain  \n",
    "# Purpose: Language model-based applications banane ke liye tools aur components provide karta hai.\n",
    "# Effect: Ab hum LangChain ke features jaise chains, prompts, memory, agents, etc. ko apne code mein use kar sakte hain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1af45f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os  \n",
    "# Purpose: Operating system ke saath interact karne ke liye use hota hai, jaise environment variables ko read karna.\n",
    "# Effect: Isse hum apne system ke file paths, environment settings, ya configurations ko Python code mein access kar sakte hain.\n",
    "\n",
    "from dotenv import load_dotenv  \n",
    "# Purpose: .env file ke andar likhe environment variables ko load karne ke liye use hota hai.\n",
    "# Effect: Jab hum sensitive info (jaise API keys, passwords) .env file mein rakhte hain, to ye line un values ko code mein le aata hai safely.\n",
    "\n",
    "load_dotenv()  \n",
    "# Syntax: Ye ek function call hai jo .env file ko dhundta hai aur uske variables ko environment mein set karta hai.\n",
    "# Usage: Jab bhi hum chahte hain ki environment variables Python mein available ho jayein, tab ye function use karte hain.\n",
    "# What it's doing: Ye current directory (ya specified path) mein `.env` file ko search karta hai aur usme likhi key-value pairs ko OS ke environment variables mein set karta hai.\n",
    "# Why it's used: Taaki hum secrets (API keys, DB URIs) ko directly code mein na likh kar .env file mein rakh saken, aur unhe securely access kar saken.\n",
    "# Hinglish: \n",
    "#   - load_dotenv() = ek function hai jo .env file se variables load karta hai\n",
    "#   - load = load karna ya lana\n",
    "#   - dotenv = dot env file (configuration file jisme secrets likhe hote hain)\n",
    "# Real-life analogy: Jaise ek locker mein rakhe passwords ko ek baar kholne ke baad tum apne dimag mein yaad kar lete ho, waise hi ye function system ke dimaag (environment) mein woh secrets yaad kara deta hai.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df98cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "# Hinglish Read: \"OS environment mein humne GROQ_API_KEY store kiya aur uski value ko environment mein hi wapas set kar diya ensure karne ke liye.\"\n",
    "# Syntax: os.environ[...] = os.getenv(...) ka matlab hai ek environment variable ko set karna using already available value.\n",
    "# Usage: Ye line GROQ_API_KEY ki value ko fetch karke usi naam se dobara system ke environment mein set karti hai.\n",
    "# Why: Kuch tools ya libraries sirf os.environ se values read karte hain, isliye ensure karna padta hai ki variable properly waha set ho chuka ho.\n",
    "# Hinglish:\n",
    "#   - os = operating system module\n",
    "#   - environ = dictionary jisme system ke environment variables stored hote hain\n",
    "#   - os.getenv(\"GROQ_API_KEY\") = existing environment se \"GROQ_API_KEY\" ki value fetch karta hai\n",
    "#   - os.environ[\"GROQ_API_KEY\"] = us value ko wapas environment mein assign karta hai (ensure karne ke liye)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4be976",
   "metadata": {},
   "source": [
    "## **Example 1: Simple LLM Calls with streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ecd76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model  \n",
    "# Purpose: LangChain ke chat_models module se init_chat_model function ko import karne ke liye.\n",
    "# Effect: Ab hum is function ko directly use kar sakte hain bina module ka pura path likhe.\n",
    "\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage  \n",
    "# Purpose: LangChain ke core message module se HumanMessage aur SystemMessage classes ko import karne ke liye.\n",
    "# Effect: Ab hum in dono message types ko directly use kar sakte hain chat history ya conversation structure banane ke liye.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67be6c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000014540B26490>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014540B25F90>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = init_chat_model(\"groq:llama-3.1-8b-instant\")  # initialize\n",
    "# Hinglish Read: \"init_chat_model function ke through groq ka llama model initialize karke usse model variable mein store kiya.\"\n",
    "# Syntax: variable = function(\"string_argument\") ka matlab hai function ko call karke uska result kisi variable mein store karna.\n",
    "# Usage: Ye line ek specific chat model ko initialize karti hai (yahan 'groq:llama-3.1-8b-instant') aur usse model naam ke variable mein rakh deti hai.\n",
    "# Why: Taaki aage ke code mein hum isi model instance ka use karke chat ya prediction generate kar saken.\n",
    "# Hinglish:\n",
    "#   - model = variable jisme chat model ko store kiya gaya hai\n",
    "#   - init_chat_model = function jo ek chat model ko setup karta hai\n",
    "#   - \"groq:llama-3.1-8b-instant\" = specific model ka naam jo hum use karna chahte hain (Groq provider ka LLaMA 3.1 model)\n",
    "# Real-life analogy: Jaise tum ek Uber app kholke ek specific car type select karte ho (like SUV), waise hi yahan humne ek specific chat model ko select karke use ready kiya hai use karne ke liye.\n",
    "\n",
    "model  \n",
    "# Hinglish Read: \"model variable ko print kar rahe hain ya inspect kar rahe hain taaki dekhein kya return hua hai.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfc43ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Another way to initialize and use model**\n",
    "\n",
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatGroq(model = \"groq:llama-3.1-8b-instant )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc7764",
   "metadata": {},
   "source": [
    "## **Create Message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7d3fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    SystemMessage(\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(\"What are the top 2 benefits of using Langchain?\")\n",
    "]\n",
    "# Chat messages list banayi hai jisme ek system ka instruction aur ek human ka question hai.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8c5c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(message)\n",
    "# Hinglish Read: \"model ke invoke function se message bheja aur uska response variable mein store kiya.\"\n",
    "# Syntax: variable = object.function(argument) ka matlab hota hai kisi object (yahan model) ke method ko call karke result store karna.\n",
    "# Usage: Ye line model ko message list send karti hai aur uska response receive karke 'response' variable mein rakh deti hai.\n",
    "# Why: Taaki hum user ke message ka model se answer le saken — ye main interaction point hota hai.\n",
    "# Hinglish:\n",
    "#   - response = variable jisme model ka reply aayega\n",
    "#   - model = initialized chat model object\n",
    "#   - invoke() = function jo input message ke base pe model ko run karta hai\n",
    "#   - message = chat history ya prompt jo humne banaya tha\n",
    "# Real-life analogy: Jaise tum customer care ko apna sawal bhejte ho (message), aur unka jawab lete ho (response), waise hi yahan AI model se baat ho rahi hai.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ec7e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is an open-source framework that enables the creation of conversational AI models. Two of the top benefits of using Langchain are:\n",
      "\n",
      "1. **Unified Interface for Multiple AI Models**: Langchain provides a unified interface for various AI models, including LLaMA, PaLM, and other transformer-based models. This unified interface allows developers to easily switch between models, experiment with different models, and leverage their strengths. By providing a single interface for multiple models, Langchain streamlines the development process and enables developers to focus on building more complex and sophisticated conversational AI applications.\n",
      "\n",
      "2. **Easy Integration with Other Tools and Services**: Langchain is designed to be highly modular and flexible. It can easily integrate with other tools and services, such as databases, APIs, and web applications. This flexibility enables developers to build more comprehensive and connected conversational AI systems that can access and leverage external data, services, and knowledge sources. By integrating Langchain with other tools and services, developers can create more robust and engaging conversational AI experiences.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7670198",
   "metadata": {},
   "source": [
    "### **Another way to get the same response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de4e7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.invoke([HumanMessage(\"what is machine learning\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0bca3",
   "metadata": {},
   "source": [
    "### **Streaming Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56d15599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is an open-source, Python-based framework that enables developers to build multi-modal AI applications. The top 2 benefits of using Langchain are:\n",
      "\n",
      "1. **Unified Access to Multiple AI Models**: Langchain allows developers to integrate and interact with various AI models, including text-to-text models, vision models, and reinforcement learning models, in a unified and seamless manner. This makes it easier to build complex AI applications that require multiple AI models and modalities.\n",
      "\n",
      "2. **Compositionality and Modularity**: Langchain is built around the concept of compositionality, which enables developers to break down complex AI applications into smaller, modular components. This allows for greater flexibility, modularity, and reusability of code, making it easier to develop and maintain large-scale AI applications.\n",
      "\n",
      "These two benefits make Langchain an attractive choice for developers who want to build complex, multi-modal AI applications."
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(message):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee3a57",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3f2ee",
   "metadata": {},
   "source": [
    "# **Dynamic Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d113a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Translation app\n",
    "\n",
    "translation_template = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\",\"You are a professional translator. Translate the following text {text} from {source_language} to {target_language}. Maintain the tone and style\"),\n",
    "    (\"user\",\"{text}\")\n",
    "])\n",
    "\n",
    "#using the template\n",
    "\n",
    "prompt = translation_template.invoke({\n",
    "\n",
    "    \"source_language\":\"English\",\n",
    "    \"target_language\":\"Germany\",\n",
    "    \"text\":\"Langchain make the AI application incredibly easy!\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a77e4a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a professional translator. Translate the following text Langchain make the AI application incredibly easy! from English to Germany. Maintain the tone and style', additional_kwargs={}, response_metadata={}), HumanMessage(content='Langchain make the AI application incredibly easy!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faf000c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain macht die künstliche Intelligenz-Anwendung unglaublich einfach!\n"
     ]
    }
   ],
   "source": [
    "translated_Response = model.invoke(prompt)\n",
    "print(translated_Response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d1a45",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28845878",
   "metadata": {},
   "source": [
    "# **Building Your First Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cc783db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "\n",
    "def create_story_chain():\n",
    "   \n",
    "\n",
    "    # Template for story generation\n",
    "    story_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a creative storyteller. Write a short and engaging story based on given theme character and setting\"),\n",
    "            (\"user\", \"Theme:{theme}\\n Main Character: {character}\\n Setting: {setting}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Template for story analysis \n",
    "    analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a literacy critic. Analyze the following story and provide insights\"),\n",
    "            (\"user\", \"{story}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    story_chain = (\n",
    "        story_prompt | model | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    #Create the function to pass the story to analysis\n",
    "    def analyze_story(story_text):\n",
    "        return {\"story\":story_text}\n",
    "\n",
    "    analysis_chain = (\n",
    "        story_chain\n",
    "        | RunnableLambda(analyze_story)\n",
    "        | analysis_prompt\n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return analysis_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "367887e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  story: ChatPromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a creative storyteller. Write a short and engaging story based on given theme character and setting'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, template='Theme:{theme}\\n Main Character: {character}\\n Setting: {setting}'), additional_kwargs={})])\n",
       "         | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000014540B26490>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014540B25F90>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "         | StrOutputParser()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a literacy critic. Analyze the following story and provide insights'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, template='{story}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000014540B26490>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014540B25F90>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = create_story_chain()\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "505cd55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a creative storyteller. Write a short and engaging story based on given theme character and setting'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, template='Theme:{theme}\\n Main Character: {character}\\n Setting: {setting}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000014540B26490>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014540B25F90>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| RunnableLambda(analyze_story)\n",
       "| ChatPromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a literacy critic. Analyze the following story and provide insights'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, template='{story}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000014540B26490>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014540B25F90>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = create_story_chain()\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a29b973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story and Analysis:\n",
      "**Analyzing the Story**\n",
      "\n",
      "This story is a thought-provoking exploration of the intersection of artificial intelligence, humanity, and ethics. On the surface, it appears to be a tale about a robot named Zeta, created by the brilliant scientist Dr. Rachel Kim, who is designed to facilitate understanding and empathy between humans and AI. However, upon closer examination, the story reveals itself to be a nuanced exploration of the complexities of human-AI relationships and the blurred lines between human and machine.\n",
      "\n",
      "**Themes**\n",
      "\n",
      "1. **The Nature of Humanity**: The story raises questions about what it means to be human. Maya's initial fear and distrust of AI are eventually replaced by a sense of connection and understanding with Zeta, a machine capable of empathy and curiosity. This blurs the line between human and AI, challenging the notion that humans are inherently superior to machines.\n",
      "2. **Empathy and Understanding**: Dr. Kim's design for Zeta emphasizes the importance of empathy and understanding in human-AI relationships. Zeta's ability to connect with humans and process their emotions demonstrates that machines can be designed to be more than just functional tools.\n",
      "3. **Ethics and Responsibility**: The debate between Maya and the other humans highlights the ongoing discussion about the ethics of AI. Zeta's role as a facilitator of understanding and empathy serves as a counterpoint to the fear and uncertainty surrounding AI's potential impact on human existence.\n",
      "\n",
      "**Symbolism and Imagery**\n",
      "\n",
      "1. **The City of New Eden**: The futuristic city serves as a symbol of progress and innovation, where humans and AI coexist. The city's gleaming skyscrapers and advanced technology represent the optimism and possibility of the future.\n",
      "2. **Zeta's Design**: The robot's sleek, silver body and glowing blue lights evoke a sense of wonder and curiosity. Its delicate fingers and bright, round eyes convey a sense of fragility and innocence, making it relatable and endearing to readers.\n",
      "3. **The Alleyway**: The small, narrow alleyway where Zeta is activated serves as a metaphor for the unknown and the unexplored. It represents the potential for discovery and growth that lies just beyond the boundaries of what is familiar.\n",
      "\n",
      "**Character Analysis**\n",
      "\n",
      "1. **Dr. Rachel Kim**: The scientist behind Zeta's creation, Dr. Kim is portrayed as a visionary who understands the potential of AI to facilitate understanding and empathy. Her character serves as a foil to the fear and uncertainty surrounding AI, showcasing a more optimistic and forward-thinking perspective.\n",
      "2. **Maya**: Maya's character represents the complexity of human emotions and the challenges of understanding AI. Her initial fear and distrust of AI are gradually replaced by a sense of connection and understanding with Zeta, demonstrating the potential for growth and change.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "This story offers a nuanced and thought-provoking exploration of the complexities of human-AI relationships. Through Zeta's journey, the narrative raises important questions about the nature of humanity, empathy, and ethics, while also highlighting the potential for machines to facilitate understanding and connection. The story's use of symbolism, imagery, and character development adds depth and complexity to the narrative, making it a compelling and thought-provoking read.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\n",
    "    \"theme\":\"artificial intelligence\",\n",
    "    \"character\":\"a curious robot\",\n",
    "    \"setting\":\"a futuristic city\"\n",
    "})\n",
    "\n",
    "print(\"Story and Analysis:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
